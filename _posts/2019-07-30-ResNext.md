---
layout: post
title:  "Daily ML - ResNeXt"
date:   2019-07-30 11:51:32 +0100
categories: jekyll update
---
These are my notes on the paper [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/abs/1611.05431). All mistakes are my own. 

## Model
A extension of ResNet which has a similar structure with groups of bottleneck residual blocks with $1 \times 1, 3\times 3 1\times 1$ convolutions and number of filters doubling when the input resolution halves as you go down the network. The key difference is that at the start of each block the channels say $N$ channels are split into $N/C$ groups where $C$ is a hyperparameter denoted *cardinality* that is kept constant across the blocks. For example $C=32$ means for $N = 128$, there are 32 blocks of $4$ channels each. These groups of channels are fed through the residual block in parallel and added together at the end.. The underlying concept is split-transform-aggregate and this block is just one realisation of this idea.   It can be implemented via group convolutions

## Properties
They try experiments on ImageNet-1K. The model improves over analogous ResNets of different depths. Cardinality is compared to other properties like width and depth. For a given model complexity increasing cardinality increases performance but shows diminshing returns. However it is found that increaseing cardinality improves performance more than increasing depth. 

## Results
It has a better single model performance on the ImageNet-1K validation set compared to various versions of ResNet, Inception and Inception-ResNet. Since models seem to saturate on ImageNet-1K they also train models on the more complex dataset ImageNet-5K and evaluate on the ImageNet-1K validation set achieving better results than analogous ResNets on both 1K-way (softmax excludes other 4K classes) and 5K-way (other 4K classes should never be predicted) evaluation. At time of publication ResNeXt models had state of the art results (with similar data augmentation) on CIFAR-10 and CIFAR-100. ResNeXts used as backbones for object detection models lead to better performance compared to analogous ResNets,