---
layout: post
title:  Word2Vec â€” 1/N
date:   2020-04-01 00:37:00
categories: jekyll update
---

Thanks to some very striking results, everyone knows about Word2Vec but the methods behind it are quite complex and subtle. In these post I will focus on the technical details since there are plenty of sources to give you intuition. 


## Data
The data consists of words and their neighbourhoods. Here is a sentence from *Emma* by Jane Austen:

"Emma's very good opinion of Frank Churchill was *a little **shaken** the following* day, by hearing that he was gone off to London, merely to have his hair cut."

The centre word is *shaken* and its neighbourbood of distance 2 in each direction consists of ["a", "little", "the", "following"]

## Model
- The parameters that the model learns are $$V$$ $$M$$-dimensional vectors, each serving as a feature for a vocabulary of $$V$$ words.
- Actually we have 2 * $$V$$ such vectors
- One represents the word when it is the centre word and th other when it is the context word. 
- The model tries to learn vectors such that a conditional probably distribution over the words is maximised. 
- There are two popular approaches, CBOW and Skip-gram

## Notation
- Let us denote the centre word vectors as $$u$$, the surrounding word vectors as $$w$$
- The context of word $i$ is the set of words $C_i$
- The vocabulary is the set of all the words use to train the model, $V$, for all of which vectors are learned

## CBOW
- Here we would like to maximise P(word \| context)
- The way we would represent it is by averaging the vectors for ["a", "little", "the", "following"] and try to make it be close to the vector for "shaken"
- We find the dot product of each of the words in the vocabulary to the average context vector and the softmax of this is interpreted as P(word \| context)

    $$ \hat{w}_c = \frac{1}{|C_i|}\sum_{c \in C_i}w_c $$
    
    $$P(u_i|\{w_c: c \in C_i\}) = \frac{\exp(u_i^T\hat{w}_c)}{\sum_{v \in V} \exp(u_v^T\hat{w}_c)}$$

## Skip-gram
- In this case we'd like to maximise P(context \| word)
- But this time the probability is approximated via Naive Bayes where it is assumed that give the centre words the probability of the surrounding words are indepedent of each other

    $$P(\{wu_c: c \in C_i\}|w_i) = \prod_{c \in C_i} P(u_c | w_i)$$ 

- The goal is again to make the surrounding word vectors similar to the centre but this time by making the "centre version" of the context vectors similar to "surrounding version" of the centre
- We still use a softmax probability but here for each surrounding vector we find the softmax over all the centre word vectors

    $$P(\{u_c: c \in C_i\}|w_i) = \prod_{c \in C_i}\frac{exp(u_c^Tw_i)}{\sum_{v \in V} \exp(u_v^Tw_i)}$$


## Negative sampling
- Straight away we can see an issue with these formulations - the need to sum across all words in the vocabulary in the denominator.
- In the next post we will discuss how to handle this with a method called **negative sampling** variants of which have found favour in other domains like self-superivised learning in computer vision. 
