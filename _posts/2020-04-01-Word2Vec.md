---
layout: post
title:  Word2Vec â€” 2/N
date:   2020-04-07 21:42:00
categories: jekyll update
---

Thanks to some very striking results, everyone knows about Word2Vec but the methods behind it are quite complex and subtle. In these post I will focus on the technical details since there are plenty of sources to give you intuition. 


## Data
The data consists of words and their neighbourhoods. Here is a sentence from *Emma* by Jane Austen:

"Emma's very good opinion of Frank Churchill was *a little **shaken** the following* day, by hearing that he was gone off to London, merely to have his hair cut."

The centre word is *shaken* and its neighbourbood of distance 2 in each direction consists of ["a", "little", "the", "following"]

## Model
- The parameters that the model learns are $$V$$ $$M$$-dimensional vectors, each serving as a feature for a vocabulary of $$V$$ words.
- Actually we have 2 * $$V$$ such vectors
- One represents the word when it is the centre word and th other when it is the context word. 
- The model tries to learn vectors such that a conditional probably distribution over the words is maximised. 
- There are two popular approaches, CBOW and Skip-gram

## Notation
- Let us denote the centre word vectors as $$u$$, the surrounding word vectors as $$w$$
- The context of word $i$ is the set of words $C_i$
- The vocabulary is the set of all the words use to train the model, $V$, for all of which vectors are learned

## CBOW
- Here we would like to maximise P(word \| context)
- The way we would represent it is by averaging the vectors for ["a", "little", "the", "following"] and try to make it be close to the vector for "shaken"
- We find the dot product of each of the words in the vocabulary to the average context vector and the softmax of this is interpreted as P(word \| context)

    $$ \hat{w}_c = \frac{1}{|C_i|}\sum_{c \in C_i}w_c $$
    
    $$P(u_i|\{w_c: c \in C_i\}) = \frac{\exp(u_i^T\hat{w}_c)}{\sum_{v \in V} \exp(u_v^T\hat{w}_c)}$$

## Skip-gram
- In this case we'd like to maximise P(context \| word)
- But this time the probability is approximated via Naive Bayes where it is assumed that give the centre words the probability of the surrounding words are indepedent of each other

    $$P(\{wu_c: c \in C_i\}|w_i) = \prod_{c \in C_i} P(u_c | w_i)$$ 

- The goal is again to make the surrounding word vectors similar to the centre but this time by making the "centre version" of the context vectors similar to "surrounding version" of the centre
- We still use a softmax probability but here for each surrounding vector we find the softmax over all the centre word vectors

    $$P(\{u_c: c \in C_i\}|w_i) = \prod_{c \in C_i}\frac{exp(u_c^Tw_i)}{\sum_{v \in V} \exp(u_v^Tw_i)}$$


## Negative sampling
- Straight away we can see an issue with these formulations - the need to sum across all words in the vocabulary in the denominator.
- The solution is to adopt a method called **negative sampling** variants of which have found favour in other areas in ML such as self-superivised learning in computer vision. 
- Consider the first pair of words which plausibly and do in fact come from Jane Austen versus the second which is  from Shakespeare

    > **true elegance** 
    >
    > -- <cite>EMMA, Jane Austen</cite>

    > **true avouch**
    >
    > -- <cite>HAMLET, William Shakespeare</cite>

- Let us say we have many such pairs, some from works by Jane Austen and the rest from other texts and for each we predict the probability that it was from Jane Austen
- Let $\mathcal{J} \in \{0, 1\}$ be an indicator variable that equals 1 if the pair is from Jane Austen and 0 otherwise,.
- The probability that pair $\(w, c)$ is from Jane Austen is given as $P(\mathcal{J}=1\vert w,c)$. 
- If it is not from Jane Austen then it is from some other text so the probability it is from some other text is  $P(\mathcal{J}=0\vert w,c) = 1 - P(\mathcal{J}=1\vert w,c)$
- Let $v_i$ be the word vector for word $i$ and $\theta the model parameters.
- The probability is expressed as follows

$$P(\mathcal{J}=1\vert w,c) = \text{sigmoid}(v_c^Tv_w) = \frac{1}{1 + \exp(-v_c^Tv_w)}$$

- We would like optimise $P(\mathcal{J}=1\vert w,c)$ for pairs from Jane Austen and $P(\mathcal{J}=0\vert w,c)$ for pairs not from Jane Austen.
- Next time we will discuss this is done and how in practice we handle the "negative pairs". 