---
layout: post
title:  Word2Vec â€” 3/N
date:   2020-04-09 09:32:00
categories: jekyll update
---

Thanks to some very striking results, everyone knows about Word2Vec but the methods behind it are quite complex and subtle. In these post I will focus on the technical details since there are plenty of sources to give you intuition. 


## Data
The data consists of words and their neighbourhoods. Here is a sentence from *Emma* by Jane Austen:

"Emma's very good opinion of Frank Churchill was *a little **shaken** the following* day, by hearing that he was gone off to London, merely to have his hair cut."

The centre word is *shaken* and its neighbourbood of distance 2 in each direction consists of ["a", "little", "the", "following"]

## Model
- The parameters that the model learns are $$V$$ $$M$$-dimensional vectors, each serving as a feature for a vocabulary of $$V$$ words.
- Actually we have 2 * $$V$$ such vectors
- One represents the word when it is the centre word and th other when it is the context word. 
- The model tries to learn vectors such that a conditional probably distribution over the words is maximised. 
- There are two popular approaches, CBOW and Skip-gram

## Notation
- Let us denote the centre word vectors as $$u$$, the surrounding word vectors as $$w$$
- The context of word $i$ is the set of words $C_i$
- The vocabulary is the set of all the words use to train the model, $V$, for all of which vectors are learned

## CBOW
- Here we would like to maximise P(word \| context)
- The way we would represent it is by averaging the vectors for ["a", "little", "the", "following"] and try to make it be close to the vector for "shaken"
- We find the dot product of each of the words in the vocabulary to the average context vector and the softmax of this is interpreted as P(word \| context)

    $$ \hat{w}_c = \frac{1}{|C_i|}\sum_{c \in C_i}w_c $$
    
    $$P(u_i|\{w_c: c \in C_i\}) = \frac{\exp(u_i^T\hat{w}_c)}{\sum_{v \in V} \exp(u_v^T\hat{w}_c)}$$

## Skip-gram
- In this case we'd like to maximise P(context \| word)
- But this time the probability is approximated via Naive Bayes where it is assumed that give the centre words the probability of the surrounding words are indepedent of each other

    $$P(\{wu_c: c \in C_i\}|w_i) = \prod_{c \in C_i} P(u_c | w_i)$$ 

- The goal is again to make the surrounding word vectors similar to the centre but this time by making the "centre version" of the context vectors similar to "surrounding version" of the centre
- We still use a softmax probability but here for each surrounding vector we find the softmax over all the centre word vectors

    $$P(\{u_c: c \in C_i\}|w_i) = \prod_{c \in C_i}\frac{exp(u_c^Tw_i)}{\sum_{v \in V} \exp(u_v^Tw_i)}$$


## Negative sampling
- Straight away we can see an issue with these formulations - the need to sum across all words in the vocabulary in the denominator.
- The solution is to adopt a method called **negative sampling** variants of which have found favour in other areas in ML such as self-superivised learning in computer vision. 
- Consider the first pair of words which plausibly and do in fact come from Jane Austen versus the second which is  from Shakespeare

    > **true elegance** 
    >
    > -- <cite>EMMA, Jane Austen</cite>

    > **true avouch**
    >
    > -- <cite>HAMLET, William Shakespeare</cite>

- Let us say we have many such pairs, some from works by Jane Austen and the rest from other texts and for each we predict the probability that it was from Jane Austen
- Let $\mathcal{J} \in \{0, 1\}$ be an indicator variable that equals 1 if the pair is from Jane Austen and 0 otherwise,.
- The probability that pair $\(w, c)$ is from Jane Austen is given as $P(\mathcal{J}=1\vert w,c)$. 
- If it is not from Jane Austen then it is from some other text so the probability it is from some other text is  $P(\mathcal{J}=0\vert w,c) = 1 - P(\mathcal{J}=1\vert w,c)$
- Let $v_i$ be the word vector for word $i$ and $\theta the model parameters.
- The probability is expressed as follows

$$P(\mathcal{J}=1\vert w,c) = \text{sigmoid}(v_c^Tv_w) = \frac{1}{1 + \exp(-v_c^Tv_w)}$$

- We would like optimise $P(\mathcal{J}=1\vert w,c)$ for pairs from Jane Austen and $P(\mathcal{J}=0\vert w,c)$ for pairs not from Jane Austen.

## Negative corpus
- We sample from the distribution that results from raising the word frequencies in the corpus to the power of $3/4$.
- The order of the weights does not change since $p^{3/4}$ is an increasing function of p for the range $[0, 1]$ but their relative sizes do.  
- These [notes](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf) give the following example of the resulting weights 
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is: 0.90 -> 0.825048

    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Constitution: 0.09 -> 0.146717

    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bombastic: 0.01 -> 0.028236

- They remark that 

    > "Bombastic" is now 3x more likely to be sampled while "is" onlywent up marginally.

- However the resulting weights are not normalized so actually would it not be as follows:

    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is: 0.90 -> 0.83

    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Constitution: 0.09 -> 0.15

    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bombastic: 0.01 -> 0.03

- But the point is that less probable words are more likely to occur in the noise corpus
- Basically the ratio of the noise to real probabilities is proportional to $\frac{p^{3/4}}{p} = p^{-1/4}$ which decreases as p increases towards 1.
- Thus that the words that were more frequent originally become relatively less frequent compared to those who were more 

## Negative sampling in practice
- Here is our example phrase again:

    >  *a little **shaken** the following* 

- For **CBOW** Swap the centre word vector $u_i$ with the vector for the vector for some other word $u_j$ sampled from the negative corpus e.g.

    > *a little **merely** the following* 

- For **skip-gram** swap the vector for each context word $u_c$ with some other word $u_k$  sampled from the negative corpus e.g.

    > *of foppery **shaken** joint they* 

- Note that the noise words in above were sampled from a mini-corpus of a few paragraphs from *Emma*. 

## What's left
- More details on the equations for
    - The two methods
    - Negative sampling
- How to modify each method to use negative sampling
- Extensions