---
layout: post
title:  Pretext-Invariant Representation Learning (PIRL)
date:   2020-01-19 16:58:54
categories: jekyll update
---

$$\newcommand{\im}{\mathbf{I}}$$
$$\newcommand{\imp}{\im^\prime}$$
$$\newcommand{\imt}{\im^t}$$
$$\newcommand{\vi}{\mathbf{v}_\im}$$
$$\newcommand{\vit}{\mathbf{v}_\imt}$$
$$\newcommand{\vip}{\mathbf{v}_\imp}$$
$$\newcommand{\mi}{\mathbf{m}_\im}$$
$$\newcommand{\mip}{\mathbf{m}_\imp}$$
$$\newcommand{\score}[2]{s\left({#1}, {#2}\right)}$$
$$\newcommand{\hexp}[2]{\exp\left(\frac{\score {#1} {#2} }{\tau}\right)}$$
$$\newcommand{\hprob}[3]{\frac{\hexp {#1} {#2} }{\hexp {#1} {#2} + \sum_{\mathbf{I}^\prime \in \mathcal{D}_N} \hexp {#2} {#3}}}$$

These are my notes on the paper [Self-Supervised Learning of Pretext-Invariant Representations](https://arxiv.org/abs/1912.01991). All mistakes are my own.

## Idea
- In the absence of labelled data, features can be learned from unlabelled data using pretext tasks whereby labels are effectively generated from the data itself, for example the image is transformed in some way and the model is trained to predict the properties of the transformation. 
- However when transformations are applied this encourages the model to learn representations that are covariant to the transformations.
- But the goals to learn representations that are invariant.
- To do so, in this paper they use the transformations for one such pretext task, the "Jigsaw" task (described below) but rather than predict the transform properties, they use a loss that encourages features of transformed versions of a given image to be similar to the original image and different from other images. 

## Model
![png]({{ site.baseurl }}/assets/PIRL/./diagram_net.png)

### Data
- The dataset consists of unlabelled images
- During training each image $\mathbf{I}$ in a minibatch, $N$ "negative" images are randomly sampled from the dataset (excluding $I$) to get $\mathbf{I}'_1,\ldots,\mathbf{I}'_N$




### Feature generation
- For each image $\mathbf{I}$, $N$ "negative" images are sampled from the dataset (excluding $I$) to get $\mathbf{I}'_1,\ldots,\mathbf{I}'_N$ 
- The inputs $\mathbf{I}$ and $\mathbf{I}'_1,\ldots,\mathbf{I}'_N$ are passed through ConvNet $\phi\_\theta$ to get features $\mathbf{v}\_\mathbf{I}$ and $\mathbf{v}\_{\mathbf{I}'_1},\ldots,\mathbf{v}\_{\mathbf{I}'_N}$
- A transformation $t$ is applied to $\mathbf{I}$ only, to get $\mathbf{I}^t$ which is then passed through $\phi_\theta$ to get $\mathbf{v}\_{\mathbf{I}^t}$

![png]({{ site.baseurl }}/assets/PIRL/./diagram_t.png)

- A head $f$ is applied to the untransformed inputs $\mathbf{v}\_\mathbf{I}$ and $\mathbf{v}\_{\mathbf{I}'_1},\ldots,\mathbf{v}\_{\mathbf{I}'_N}$

![png]({{ site.baseurl }}/assets/PIRL/./diagram_f.png)

- A head $g$ is applied to the transformed input $\mathbf{v}_\mathbf{I}^t$

![png]({{ site.baseurl }}/assets/PIRL/./diagram_g.png)

- These vectors are then used to make predictions that are fed into a loss function.

### Memory bank
- Instead of generating the features for the "negative" images $\mathbf{I}'$ each time, 
- Consists of features $m_\mathbf{I}$ for each image $\mathbf{I}$.
- $m_I$ is initialised as $f(\mathbf{v}_\mathbf{I})$
- During training, we only generate features for $\mathbf{I}$ and we use from the memory bank features for $\mathbf{I}'$, $\mathbf{m}_\mathbf{I}$ instead of generating them online 
- The memory bank features of the "positive" image $\mathbf{m}\_\mathbf{I}$, are updated with $f(\mathbf{v}_\mathbf{I})$, via EMA 

## Constrastive loss
### Loss function 
- The goal is (1) to make the features image $\mathbf{I}$ similar to the features of its transformed version whilst (2) ensuring these features do not change too much from the memory bank features.
- The loss consists of two parts corresponding to (1) and (2) above:
![png]({{ site.baseurl }}/assets/PIRL/pirl_eq5.png)
- The loss function is given as

![png]({{ site.baseurl }}/assets/PIRL/pirl_eq4.png)

- The function $h$ is given as 

![png]({{ site.baseurl }}/assets/PIRL/pirl_eq3.png)


where $s$ is the cosine similarity

- This now becomes for (1):
$$h(\mi, g(\vit))= 
\hprob{\mi} {g(\vit)} {\mip} [*]$$ 

### Probability for positives
- $h$ is the softmax with temperature $\tau$ over the following $N+1$ terms

    $$\score {\mi} {g(\vit)}, \left\{\score {\mip} {g(\vit)}: \mathbf{I}^\prime \in \mathcal{D}_N\right\}$$
    
    
- Similarly for the second loss the softmax over

    $$\score {\mi} {f(\vi)}, \left\{\score {\mip} {f(\vi)}: \mathbf{I}^\prime \in \mathcal{D}_N\right\}$$

   
- Then $h({\mathbf{m}_\mathbf{I}}, {g(\vit)})$ can be interpreted as the probability that $\mi$ and ${g(\vit)}$ come from the same image relative to the remaining pairs of features. 
- In the second case the probability is that $\mi$ and ${f(\vi)}$ (i.e. the present and the earlier features) come from the same image relative to the remaining pairs.

### Probability for negatives
- The loss has the term $h(g(\vit), f(\vip))$ which becomes $h(g(\vit), \mip)$ since we never use $f(\vip)$ for the "negative" images.  
- Consider the equation $[*]$ repeated below:

$$h(\mi, g(\vit))= 
\hprob{\mi} {g(\vit)} {\mip}$$ 

- Since now we have $h(g(\vit), \mip)$, assuming the substitution $\mi \rightarrow g(\vit), g(\vit) \rightarrow \mip$ you get

$\newcommand{\Nexpt}{N\exp\left(\frac{1}{t}\right)}$

$$h(g(\vit), \mip)= 
\hprob{g(\vit)} {\mip} {\mip}
\\= \frac{\hexp{g(\vit)} {\mip}}{\Nexpt + \hexp{g(\vit)} {\mip}}  
\\= \frac{1}{\Nexpt\cdot \exp\left(-\frac{\score {g(\vit)} {\mip} }{\tau}\right) + 1} 
\\= \frac{1}{\exp\left(-\left(\frac{\score {g(\vit)} {\mip} - 1}{\tau} - \log N\right)\right) + 1} 
\\=\text{sigmoid}\left(\frac{\score {g(\vit)} {\mip} - 1}{\tau} - \log N\right)
\\\equiv\text{sigmoid}\left(a\cdot{\score {g(\vit)} {\mip}} + b\right)$$


where in the last line we have written it as the sigmoid of a linear transform of $s$ with

$$a = \frac{1}{\tau}\\
b =-\frac{1}{\tau} - \log N$$ 

- The constant term $N\exp(1/\tau)$ comes about since $\score {\mip} {\mip} = 1$ 
- Since $A$ is positive, $0 \leq h(g(\vit), \mip) \leq 1$
- Analogous to sigmoid as the features become very similar the exponential term tends to $0$ and $h$ tends to $1$ and vice versa 

### Difference in the probabilities 
- The probability for the negatives depends only on how similar are the pair $g(\vit), \mip$ or, for the second loss, $f(\vi), \mip$.
- Whereas the probability for the positives depends on how similar are $\mi, g(\vit)$ relative to all the $\mip, g(\vit)$.
- The need for a different kind of probability in this step arises because we are only concerned with how $\imp$ relates to $\im$ and $\imt$ and not with each other.

## Jigsaw task

- This is the transformation used as $t$ and merely consists of cropping 9 patches from the image which comprise $\vit$, then independently applying $\phi_\theta$ to each patch to get features for each, then merging these features via $g$.

## Hyperparameters
- $\lambda = 0.5$ (so the loss is just average of the two terms)
- $\tau = 0.07$
- Weight of 0.5 for exponential moving average

## How to use this model
- Once it has been trained with the loss, the features from the model can be used to train simpler models for various tasks such as classification and object detection.
- The simpler models could be trained, for instance, with smaller quantities of labelled data but get much better results than without the self-supervised pre-training.
- Similarly more complex models pre-trained in this manner can be fine-tuned with less data and achieve better performance. 
