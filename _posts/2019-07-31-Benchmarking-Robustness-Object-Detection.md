---
layout: post
title:  "Daily ML - Benchmarking Robustness in Object Detection"
date:   2019-07-31 11:51:32 +0100
categories: jekyll update
---
These are my notes on the paper [Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming](https://arxiv.org/abs/1907.07484). All mistakes are my own. 

## Introduction 
In this paper they investigate the performance of object detection models on corrupted versions of Cityscapes, MS Coco and Pascal VOC datasets. They have created a robustness benchmark dataset with 15 different kinds of corruption each with 5 levels of severity. The corruptions are sorted into 4 groups: noise, blur, digital and weather. These are not used in training as their purpose is to determine the model's robustness to previously unseen corruptions. 

## Evaluation 
Models are evaluated by finding the average precision (AP) for each corruption at each severity and the scores are averaged across all corruptions and severities to give the mean performance under corruption score, $mPC$. The score is also obtained for the clean dataset, $P_\text{clean}$ and the relative performance under corruption, $rPC$, is the ratio of the two $rPC = mPC/P_\text{clean}$.

## Performance on corrupted data 
Models that are simply trained on the clean training data experience a significant performance drop on the corrupted data. In general it turns out that corrupted performance correlates with clean performance. The state of the art model (at time of publication), Hybrid Task Cascade does significantly better than all the others. It also appears that the feature representations from the model backbone matter more than the head architecture since giving Faster R-CNN a better backbone yields greater performance increase compared to adding a sophisticated cascade head. It also turns out for individual corruptions larger corruption size measured in root mean square error (RMSE) does not necessarily lead to a greater drop in model performance. 

## Style transfer to improve performance  
Their approach is to train on stylised data generated by transferring styles from Kaggle's Painter by Numbers dataset using AdaIN. They try training on just stylised data and on a dataset consisting the original images plus a stylised version of each. Whilst training on the combined dataset leads to better $mPC$, training on the stylised only dataset leads to better $rPC$ on all the three datasets. 