---
layout: post
title:  SimCLR
date:   2020-02-16 10:37:00
categories: jekyll update
---
These are my notes on the paper [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/pdf/2002.05709.pdf). All mistakes are my own.

## The approach
- The model is trained on a large unlabelled dataset using the contrastive loss.
- It can be used to generate features that can be used in tasks downstream without having to train a large supervised model. 
- Typically the model is evaluated by training a linear classifier using these features

##  NT-Xent loss
- For each vector $x_k, k=1, \ldots, N$, you apply a pair of augmentations to get $\tilde{x}\_{2k - 1}, \tilde{x}\_{2k}$ result in $N$ pairs of vectors. 

![png]({{ site.baseurl }}/assets/SimCLR/simclr_1.jpg)

- For $x_k$, the pair $\tilde{x}\_{2k - 1}, \tilde{x}\_{2k}$ is consider "positive" whilst all pairs $\tilde{x}\_{2\kappa' - 1}, \tilde{x}\_{2\kappa,, \kappa \neq k}$ are considered "negative".
- For each of these features are obtained from a base "backbone" model (such as a ResNet architecture) denoted $f$ and these are the non-linearly projected via a shallow model $g$ to an embedding space resulting in vectors $z_i$ for each $\tilde{x}_i$.
- For each $\tilde{z}\_{i}$, the pairwise cosine similarity $s_{ij}$ is found for all other vectors $\tilde{z}_{i, i\neq j}$, resulting in a $2N-1$-d vector of similarities.
- We find the softmax of this vector and for each odd $i$ we keep just the softmax corresponding to $s_{i, i+1}$ and for each even i the one corresponding to $s_{i, i=}$
- We interpret this as the probability that for $\tilde{z}\_{i}$, $\tilde{z}\_{i+1}$ or $\tilde{z}_{i-1}$, as the case may be, is positive relative to all the *other* $\tilde{z}\_{j}$, apart from $\tilde{z}\_{i}$ itself.

![png]({{ site.baseurl }}/assets/SimCLR/simclr_2.jpg)

- From this point it is simply a case of training with a cross entropy where these probabilities are the predictions and the labels are all ones. 
- There no negative labels as such but negativity features implicitly in the denominator of the softmax since to make the probability larger since for a given images, different images should have embeddings that are less similar compared to embeddings of augmented versions of the same image. 
- The pretext task here is essentially to predict whether the embedding of one augmentation on an image is positive to the embedding of another augmentation relative to the embeddings for all the other images. 


## Benefits of the loss function
- Using CE-loss automatically does hard-negative mining with the right temperature.
- If a negative example has a high probability it leads to higher contribution to the gradient in the opposite direction 
- The input vectors to the loss are $l_2$-normalised 
- Adding semi-hard negative mining helps performance when using other losses but NT-Xent is still the best


## Implementation details
- Augmentations
    - Random crop and resize
    - Random flip
    - Colour distortion
    - Gaussian blur
- $f$ = ResNet50 as base encoder 
- $g$ = 2-layer MLP to project representations to a 128-dimensional space
- NT-Xent loss
- LARS optimiser
- Weight decay of $10^{-6}$
- Batch size of 4096 for 100 epochs
- Learning rate
    - Linear learning rate scaling `lr = 0.3 * batch_size / 256`
    - Linear warmup for first 10 epochs
    - Cosine decay schedule without restarts


## Differences between supervised and unsupervised
They following which can benefit supervised training were found to a have a greater effect on improving performance of unsupervised models trained in this paper compared to several supervised models:
- Bigger i.e. wider and / or deeper models
- Non-linear
- Larger batch sizes
     - Helps models to converge after fewer iterations
     - To put it differently when training for fewer iterations larger batch sizes lead to better performance
     - The reason for the effect for unsupervised learning is that availability of more negative examples
     - Longer training also provides more negative examples so compensates for smaller batch size

## Effect of augmentations
- You can typically tell which patch an image comes from by its colour histogram since most patches from a given image have a similar colour distribution (presumably they mean most sufficiently large patches).
- To prevent the model from exploiting this short-cut, colour distortion should be applied not just cropping. 
- Unsupervised learning benefits more from stronger colour augmentations than supervised:
    - They compare supervised ResNet50 with a linear classifier trained with unsupervised ResNet50 features
    - They compared the effect in each case of applying colour augmentations of various strengths and AutoAugment
    - For supervised, AutoAugment is better than all the strengths and increasing the strength makes performance worse
    - For unsupervised increasing strength improves performance, surpassing AutoAugment at higher strengths
- Different augmentations applied on their own or in pairs,
    - They are applied both orders e.g. colour then noise, noise then color:
    - In some cases the order does matter - for coloir and noise, coloir followed by noise does between than noise followed by colour
- For the above since images are of different sizes cropping is always done with other transform, they study the effect of augmentations applied on their own 
    - Randomly crop and resize each image to the same resolution
    - Apply transform only to one branch of the framework

## Results
- For all these only $f$ is used and it is either fine-tuned or 
- Semi-supervised
    - Model is finetuned on a small fraction (1% or 10% of the data)
- Transfer learning
    - Classification experiments are done on 12 other image datasets
    - An ImageNet trained ResNet50 is used, either frozen or fine-tuned.
    - It does as well or better in 7 out of 12 datasets when the backbone is frozen.
    - It does as well or better in 10 out of 12 datasets when the backbone is finetuned. 
- Linear evaluation 
    - Beats all earlier methods
    - An unsupervised Wide ResNet50 with width multiplier 4x achieves ImageNet top-1 accuracy of 76.5 % in linear evaluation that is comparable to the result from supervised ResNet50 (76.3 %).
