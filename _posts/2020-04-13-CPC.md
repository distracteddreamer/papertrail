---
layout: post
title:  "[WIP] Representation Learning with Contrastive Predictive Coding"
date:   2020-04-13 12:45:00
categories: jekyll update
---
These are my notes on the paper [Representation Learning with Contrastive Predictive Coding](https://arxiv.org/pdf/1807.03748.pdf). All mistakes are my own.

## Introduction
- The idea behind this method is to learn good task-agnostic representations from unlabelled data.
- A model pre-trained on say on a classification task on one dataset can typically extract features that enable it to generalise well across datasets but it is harder to extract features that generalise well across tasks like localisation. 

## Method
### Sequences
- You consider the model as sequence.
- Even if it is not strictly sequential you can represent it as a sequence for example dividing an image into parts and going through them one at a time. 
- For each element $x_t$ of the sequence, features are obtained from a backbone encoder architecture $g_\text{enc}$, yielding $z_t = g_\text{enc}(x_t)$
- The encoder $g_\text{enc}$ can be any architecture appropriate for the domain e.g. ResNets for images and typically is not pre-trained and once it has been trained via this method, its outputs can be used for downstream tasks. 

### Contexts and futures 
- For a given position $t$ in the sequence you then combine all the $z_{\tau \le t}$ via an autoregressive model (one which outputs a value for each element of the sequence which is then fed back to the model to produce the next output) to get a context vector $c_t$.
- Then you define a "future neighbourhood" for $t$ which could all the elements upto $T$ positions in the future. 
- The idea is that you should be able to predict "future features" i.e. features of elements at a given locations in that neighbourhood using the context $c_t$.
- In practice you don't actually predict future features. Instead we turn it into a classification task.

### Positives and negatives
- For a given location $t$ we consider a set of $N$ features. Exactly one of these features is from the neighbourhood of $t$ and thus regarded as positive. The rest, which are also from the future, but are not from this neighbourhood are denoted as negative samples. 
- Let the positive sample be denoted as $x_i$. The task is to predict whether $z_i$ is positive or not relative to $x_t$. 
- For each sample you obtain a score for each of the pairs $(z_j, c_t), i = 1, \ldots, N$. In the paper they project $c_t$ a different vector for each position in the neighbourhood i.e. $W_kc_t$ for $k$ steps into the future and then take the dot product with $z_j$ yielding:

    $s_{j,t}^k = z_j^T W_kc_t$

- Next time we will discuss how the positive sample is considered to come from the future distribution $p(x_{t + k}\vert c_t)$ conditioned on the context whilst the negative sample is considered to come from $p(x_{t + k})$, the marginal future distribution and how the model learns representations in a way that maximally preserves mutual information of $c$ and $x$.