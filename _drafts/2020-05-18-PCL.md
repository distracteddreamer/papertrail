---
layout: post
title:  Prototypical Contrastive Learning
date:   2020-05-18 05:21:00
categories: jekyll update
---
## Introduction

This is yet another unsupervised learning approach. As usual, features $v_i$ are obtained for unlabelled data $x_i$ using one or more feature extractors $f(x_i;\theta)$. Here there are two feature extractors, the main one $f(x_i;\theta)$ and a "momentum" one $f_m(x_i;\theta)$ where $f_m$ is essentially the same model but parameterised with the moving average of $\theta$.

The paper introduces a new training approach. 

## Loss function
As usual the goal is to find parameters to maximise the log-likehood of the data'

[Equation 2]

The $N$ datapoints $x_i$ are considered to be related to $K$ latent variables $c_k$ leading to the following expression for the log-likelihood

[Equation 3]

Instead of optimising the above directly we optimise the following lower bound which comes about via Jensen's inequality


$Q(c_k)$ is a distribution over the latent variables. The inequality holds with equality only when $\frac{p(x_i, c_k|\theta)}{Q(c_k)}$ is constant at which point $Q(c_k) = p(c_k|x_i, \theta)$.

## EM-algorithm
This approach as an Expectation-Maximisation (EM) problem. 

### E-step
In this step, the weights are fixed in their latest configuration and get features $v_{m,i}$ for all the datapoints from the momentum encoder. You then cluster the features via $k$-means clustering. The distribution $p(c_k|x_i, \theta)$ is then defined with respect to the clusters as $\mathbb{1}(x_i \in c_k)$

### M-step
This step involves the usual weight updating via maximising the log-likelihood with the following substitions:

    - $p(c_k| x_i, \theta) = \mathbb{1}(x_i \in c_k)$ based on the expression obtained in the $E$-step
    - $p(c_k|\theta) =\frac{1}{K} \implies p(x_i, c_k|\theta) = 
    p(c_k|\theta)p(x_i| c_k, \theta) = \frac{1}{k}p(x_i| c_k, \theta)$ assuming that the prior probability for $c_k$ is uniform.

This leads to the following optimisation step

    Equation [10]

## Recipe