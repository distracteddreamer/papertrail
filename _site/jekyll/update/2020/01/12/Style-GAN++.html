<!DOCTYPE html>
<html lang="en"><head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
      <title>Paper Trail</title>
      <meta name="generator" content="Jekyll v3.8.5" />
      <meta property="og:title" content="Your awesome title" />
      <meta property="og:locale" content="en_US" />
      <meta name="description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description." />
      <meta property="og:description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description." />
      <link rel="canonical" href="http://localhost:4000/papertrail/" />
      <meta property="og:url" content="http://localhost:4000/papertrail/" />
      <meta property="og:site_name" content="Your awesome title" />
      <script type="application/ld+json">
      {"name":"Your awesome title","description":"Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.","@type":"WebSite","url":"http://localhost:4000/papertrail/","headline":"Your awesome title","@context":"https://schema.org"}</script>
    
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          jax: ["input/TeX", "output/HTML-CSS"],
          tex2jax: {
            inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
            displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
            processEscapes: true,
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
          }
          //,
          //displayAlign: "left",
          //displayIndent: "2em"
        });
      </script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


      <!-- End Jekyll SEO tag -->
      <link rel="stylesheet" href="/papertrail/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/papertrail/feed.xml" title="Your awesome title" /></head>

<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/papertrail/">Paper Trail</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/papertrail/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Style-GAN++ — Scribblings</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-01-12T00:00:00+00:00" itemprop="datePublished">Jan 12, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.layers.convolutional</span> <span class="kn">import</span> <span class="n">Conv</span>
<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="kn">import</span> <span class="n">nn</span>
</code></pre></div></div>

<h2 id="revised-style-block">Revised style block</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">StyleBlock</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">conv_kwargs</span><span class="p">,</span> <span class="n">dense_kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="o">**</span><span class="n">conv_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">conv_kwargs</span><span class="p">[</span><span class="s">'filters'</span><span class="p">],),</span>
                             <span class="n">initializer</span><span class="o">=</span><span class="s">'zeros'</span><span class="p">,</span>
                             <span class="n">trainable</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_mod</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="o">**</span><span class="n">dense_kwargs</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">style</span><span class="p">,</span> <span class="n">noise</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_mod</span><span class="p">(</span><span class="n">style</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">y</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_std</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">noise</span>
        <span class="c1"># TODO: confirm these are the axis
</span>        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>Let there be $C$ filters in the input and $F$ filters in the output. Let the kernel for the $f$-th filter $w_f$ have shape $H \times W$ and let $p$ be a $C \times H \times W$ size patch of the input. The pixel in the $f$-th channel of the output $a_{fi’j’}$ resulting from this input patch convolved with the $f$-th kernel is given as</p>

<script type="math/tex; mode=display">a_{fi'j'} = \sum_{c=0}^{C-1}\sum_{i=0}^{H-1}\sum_{j=0}^{W-1}w_{fcij}p_{cij}</script>

<p>In the modulation step we scale the output feature maps of the conv layer by a $F$-dimensional style vector $y$. But we could equivalently achieve this result by scaling the conv kernel so that instead of $a’<em>{fi’j’} = y_fa</em>{fi’j’}$ we have</p>

<script type="math/tex; mode=display">w'_f = y_fw_f \\
a'_{fi'j'} = \sum_{c=0}^{C-1}\sum_{i=0}^{H-1}\sum_{j=0}^{W-1}w'_{fcij}p_{cij} \\</script>

<p>If $p_{cij} \sim \mathcal{N}(\mu, 1)$ (i.i.d with std of 1), then std of the $f$-channel activations is</p>

<script type="math/tex; mode=display">\text{Var}(a_f) = \text{Var}\left(\sum_{cij}w_{fcij}p_{cij}\right)
= \sum_{cij}\text{Var}(w_{fcij}p_{cij})
= \sum_{cij}w_{fcij}^2\text{Var}(p_{cij})
= \sum_{cij}w_{fcij}^2
\\\sigma_f = \sqrt{\text{Var}(a_f)} = \sqrt{\sum_{cij}w_{fcij}^2}</script>

<p>Since in the norm step the feature maps are scaled by their std, if $p_{cij} \sim \mathcal{N}(\mu, 1)$, then the outputs are scaled by the $L_2$ norm of the weights.</p>

<p>Similarly the variance scaling can be incorporated into the kernel</p>

<script type="math/tex; mode=display">w''_f = w'_f / ||w'_f||_2\\
a''_{fi'j'} = \sum_{c=0}^{C-1}\sum_{i=0}^{H-1}\sum_{j=0}^{W-1}w''_{fcij}p_{cij} \\</script>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">StyleConv</span><span class="p">(</span><span class="n">Conv</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s">'eps'</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">StyleConv</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">_mod_demod_kernel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">style</span><span class="p">,</span> <span class="n">noise</span><span class="p">):</span>
        <span class="n">kernel</span> <span class="o">=</span>  <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">*</span> <span class="n">style</span>
        <span class="n">l2_norm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">kernel</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]),</span>
                          <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span> <span class="o">/</span> <span class="p">(</span><span class="n">l2_norm</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">kernel</span>
    
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">style</span><span class="p">,</span> <span class="n">noise</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convolution_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mod_demod_kernel</span><span class="p">(</span><span class="n">style</span><span class="p">))</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">+</span> <span class="n">noise</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">==</span> <span class="s">'channels_first'</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="c1"># nn.bias_add does not accept a 1D input tensor.
</span>                    <span class="n">bias</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">filters</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
                    <span class="n">outputs</span> <span class="o">+=</span> <span class="n">bias</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">outputs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s">'NCHW'</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s">'NHWC'</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">outputs</span>
    
<span class="k">class</span> <span class="nc">StyleConv2D</span><span class="p">(</span><span class="n">StyleConv</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s">'rank'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">StyleConv2D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="lazy-regularisation">Lazy regularisation</h2>
<ul>
  <li>In the interests of (speed ?) reg terms are executed only after every $k$ training iterations</li>
  <li>For $k$ iterations usual loss is used</li>
  <li>Then for 1 iteration reg loss is used</li>
  <li>Adam is shared with its parameters adjusted since for every $k$ iterations there are now $k + 1$ iterations</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_adam_lazy_reg</span><span class="p">(</span><span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">lam</span><span class="p">,</span> <span class="n">n_iters</span><span class="p">):</span>
    <span class="n">factor</span> <span class="o">=</span> <span class="n">n_iters</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_iters</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">beta1</span><span class="o">=</span><span class="n">beta1</span><span class="o">**</span><span class="n">factor</span><span class="p">,</span> 
                              <span class="n">beta2</span><span class="o">=</span><span class="n">beta2</span><span class="o">**</span><span class="n">factor</span><span class="p">,</span> 
                              <span class="n">lam</span><span class="o">=</span><span class="n">factor</span><span class="o">*</span><span class="n">lam</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="path-length-regularisation">Path length regularisation</h2>

<p>I think that since $\mathbf{y}$ are referred to as random images, we have that $\nabla_\mathbf{w}(g(\mathbf{w})\cdot \mathbf{y}) = \mathbf{J}<em>\mathbf{w}^T\mathbf{y} + g(\mathbf{w})\nabla</em>\mathbf{w}\mathbf{y} = \mathbf{J}_\mathbf{w}^T\mathbf{y}$ because if $\mathbf{y}$ is some random image from $\mathcal{N}(0,\mathbf{I})$ its gradients with respect to $\mathbf{w}$ are 0.</p>

<p>This contrivance used as it gives us $\mathbf{J}_\mathbf{w}^T\mathbf{y}$ without having to find the Jacobian.</p>

<p>The expected value of  $\lVert \mathbf{J}_\mathbf{w}^T\mathbf{y} \rVert_2^2$:</p>

<script type="math/tex; mode=display">E_{\mathbf{y}}\left[\left\lVert\mathbf{J}_\mathbf{w}^T\mathbf{y}\right\rVert_2^2\right]
= E_{\mathbf{y}}\left[\sum_a\left(\sum_b{\mathbf{J}_{\mathbf{w},ab}}\mathbf{y}_b\right)^2\right]
= E_{\mathbf{y}}\left[\sum_a\left(\sum_b\sum_{b'}{\mathbf{J}_{\mathbf{w},ab}}\mathbf{y}_b
{\mathbf{J}_{\mathbf{w},ab'}}\mathbf{y}_{b'}\right)\right]
\\ = \sum_a\sum_b {\mathbf{J}_{\mathbf{w},ab}}^2 E_{\mathbf{y_b}}\left[\mathbf{y}_b^2\right]
+
\sum_a\sum_{b, b\neq b'}\sum_{b'}{\mathbf{J}_{\mathbf{w},ab}}
{\mathbf{J}_{\mathbf{w},ab'}}E_{\mathbf{y_b}}\left[\mathbf{y}_{b}\right]E_{\mathbf{y_{b'}}}\left[\mathbf{y}_{b'}\right]
\\ = \sum_a\sum_b {\mathbf{J}_{\mathbf{w},ab}}^2 = \text{tr}\left({\mathbf{J}_{\mathbf{w}}}{\mathbf{J}_{\mathbf{w}}}^T\right)</script>

<p>Since the elements of $\mathbf{y}$ are independent the expectation of each element can be found separately. We also rely on the following:</p>

<script type="math/tex; mode=display">E[\mathbf{y_b}] = 0 \implies E[\mathbf{y_b}^2] = \text{Var}(\mathbf{y_b}) = 1</script>

<p>Minimising the above makes the elements of the Jacobian small. In practice a value $a$ is subtracted.</p>

<script type="math/tex; mode=display">E_{\mathbf{y}}\left[\left(\left\lVert\mathbf{J}_\mathbf{w}^T\mathbf{y}\right\rVert_2 - a\right)^2\right]</script>

<p>The value $a$ is made to be the exponential moving average of $\left\lVert\mathbf{J}_\mathbf{w}^T\mathbf{y}\right\rVert_2$. Let us think about how this regularisation might work:</p>

<ul>
  <li>At any step $J_w$ depends on the weights at that point.</li>
  <li>Say that at step $t$ $\left\lVert\mathbf{J}<em>\mathbf{w}^T\mathbf{y}\right\rVert_2^2$ is quite different from $a$ which pushes up the loss — simplistically this encourages the weights to to push $\left\lVert\mathbf{J}</em>\mathbf{w}^T\mathbf{y}\right\rVert_2^2$ towards $a$.</li>
  <li>The weights will also be influenced by the other losses.</li>
  <li>At the next step $a$ has been pushed up a bit so if $\left\lVert\mathbf{J}_\mathbf{w}^T\mathbf{y}\right\rVert_2^2$ has decreased too much, then in the step after it will be pushed up again</li>
  <li>Possibly the term will become more stable, staying near to some value of $a$ as the network as a whole becomes more stable.</li>
  <li>What this value is depends on the weights that work well for the task as a whole.</li>
  <li>However the effect of the regularisation is to exert some control on the Jacobian.</li>
</ul>

<h2 id="architecture">Architecture</h2>

<ul>
  <li>Dimensionality of $Z$ and $W$: 512</li>
  <li>Mapping network architecture
    <ul>
      <li>8 fully connected layers</li>
      <li>100$\times$ lower lr</li>
    </ul>
  </li>
  <li>Architecture
    <ul>
      <li>leaky ReLU, $\alpha=0.2$</li>
      <li>bilinear filtering in all up/downsampling layers (?)</li>
      <li>minibatch std layer at end of discriminator</li>
    </ul>
  </li>
  <li>Training
    <ul>
      <li>equalized lr for all trainable params (?)</li>
      <li>EMA of generator weights</li>
      <li>style mixing reg</li>
      <li>non-saturating logistic loss with $R_1$ reg</li>
      <li>Adam optimiser ($\beta_1 = 0, \beta_2 = 0.99, \epsilon = 10^{-8}$)</li>
      <li>batch_size 32</li>
      <li>8 GPUs</li>
    </ul>
  </li>
</ul>

  </div><a class="u-url" href="/papertrail/jekyll/update/2020/01/12/Style-GAN++.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/papertrail/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Paper Trail</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Paper Trail</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Tracking my journey through AI </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
