<!DOCTYPE html>
<html lang="en"><head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
      <title>Paper Trail</title>
      <meta name="generator" content="Jekyll v3.8.5" />
      <meta property="og:title" content="Your awesome title" />
      <meta property="og:locale" content="en_US" />
      <meta name="description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description." />
      <meta property="og:description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description." />
      <link rel="canonical" href="http://localhost:4000/papertrail/" />
      <meta property="og:url" content="http://localhost:4000/papertrail/" />
      <meta property="og:site_name" content="Your awesome title" />
      <script type="application/ld+json">
      {"name":"Your awesome title","description":"Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.","@type":"WebSite","url":"http://localhost:4000/papertrail/","headline":"Your awesome title","@context":"https://schema.org"}</script>
    
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          jax: ["input/TeX", "output/HTML-CSS"],
          tex2jax: {
            inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
            displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
            processEscapes: true,
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
          }
          //,
          //displayAlign: "left",
          //displayIndent: "2em"
        });
      </script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


      <!-- End Jekyll SEO tag -->
      <link rel="stylesheet" href="/papertrail/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/papertrail/feed.xml" title="Your awesome title" /></head>

<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/papertrail/">Paper Trail</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/papertrail/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Sparse Transformer</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-07-29T11:51:32+01:00" itemprop="datePublished">Jul 29, 2019
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>These are my notes on the paper <a href="https://arxiv.org/abs/1904.10509">Generating Long Sequences with Sparse Transformers</a>. All mistakes are my own.</p>

<h2 id="motivation">Motivation</h2>
<p>In the standard Transformer (see this excellent tutorial to understand it in depth) self-attention,each position attends to all previous positions which is $O(n^2)$ making it prohibitively expensive for long sequences. It turns out that in practice at least some of the learned attention maps are quite sparse, sometimes attending only to specific points for certain categories or paying attention only to some columns or rows - the latter suggestive of some kind of factorisation of the attention maps. This motivates the design of sparse attention heads that for each point only attend to a subset of previous locations. However there are several such heads each with a different subset so that via a combination of heads, a path exists from each previous location to the present one. The complexity is now $O(n\sqrt(n))$ rather than $O(n^2)$.</p>

<h2 id="model">Model</h2>
<p>For each attention type a cell attends to a fixed number of previous cells that depend on its location and via the cells to which a cell attends it gets summarised information from other cells. They use two attention heads and discuss three ways in which this could be implemented.</p>

<ol>
  <li>Each residual block has one attention type and blocks with different attention types are interleaved. 2. The subset pixels of each attention type are combined across all the types and used in a single <em>merged</em> head.</li>
  <li>Different attention patterns are run in parallel and the result is concatenated along the feature dimension - the attention types could be</li>
</ol>

<p>The architecture was modified to use a different kind of residual block and embedding layer compared to the original.</p>

<h2 id="results">Results</h2>
<p>The model achieved superior performance for density modelling tasks on CIFAR10 compared to other models in a briefer time. For density modelling on EnWik8  it was as good as the best model (Transformer-XL 277M) which used over twice as many parameters as the Sparse Transformer. The model also performed better than other models larger images in ImageNet 64x64. In addition they trained models for a classical music dataset but did not compare to other models because of the lack of details of dataset processing but obtained qualitatively good results of clips of around 5 seconds in length.</p>


  </div><a class="u-url" href="/papertrail/jekyll/update/2019/07/29/Sparse-Transformer.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/papertrail/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Paper Trail</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Paper Trail</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Tracking my journey through AI </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
